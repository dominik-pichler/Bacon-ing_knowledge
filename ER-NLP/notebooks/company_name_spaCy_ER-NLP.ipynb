{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import spacy\n",
    "import de_dep_news_trf\n",
    "import pandas as pd\n"
   ],
   "id": "6c5eef2ce8b3d62a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_query(database_path, query):\n",
    "    # Connect to the SQLite database\n",
    "    # If the database does not exist, it will be created\n",
    "    connection = sqlite3.connect(database_path)\n",
    "    \n",
    "    try:\n",
    "        # Create a cursor object using the connection\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Execute the SQL query\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Fetch all results from the executed query\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        # Turning list of tuples to list of strings for further processing\n",
    "        return [str(t[0]) for t in results]\n",
    "\n",
    "            \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        # Close the connection to the database\n",
    "        if connection:\n",
    "            connection.close()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6801af4d24dd5a1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "db87c61ce994d750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# spaCy German",
   "id": "b2c21852902aab30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!python -m spacy download de_core_news_lg",
   "id": "950b6a6ff57c92a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_person_names(text, nlp_model):\n",
    "    # Process the text using the specified language model\n",
    "    doc = nlp_model(text)\n",
    "    # Extract person names\n",
    "    z = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n"
   ],
   "id": "f92808bc5f761455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T12:10:28.012750Z",
     "start_time": "2024-10-01T12:10:26.205331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "# Load the German transformer model\n",
    "de_dep_nlp = spacy.load('de_core_news_lg')\n",
    "\n",
    "database_path = 'openregister.db'  # Path to your SQLite database file\n",
    "query = 'SELECT name FROM company LIMIT 100;'  # Replace 'your_table' with your actual table name\n",
    "\n",
    "company_names = run_query(database_path, query)\n",
    "\n",
    "results = []\n",
    "for company_name in company_names:\n",
    "    # Create a spaCy document for the company name\n",
    "    doc = de_dep_nlp(company_name)\n",
    "\n",
    "    # Iterate over each token and perform NER\n",
    "    for token in doc:\n",
    "        # Store each token's text and entity type in a dictionary\n",
    "        results.append({\n",
    "            'Company Name': company_name,\n",
    "            'Token': token.text,\n",
    "            'Entity': token.ent_type_ if token.ent_type_ else 'No entity'\n",
    "        })\n",
    "\n",
    "# Convert results to a pandas DataFrame for better structure and analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df_results.to_csv('ner_results.csv', index=False)"
   ],
   "id": "c300835a736ac92d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Company Name                  Token     Entity\n",
      "0               olly UG (haftungsbeschränkt)                   olly        ORG\n",
      "1               olly UG (haftungsbeschränkt)                     UG        ORG\n",
      "2               olly UG (haftungsbeschränkt)                      (  No entity\n",
      "3               olly UG (haftungsbeschränkt)     haftungsbeschränkt  No entity\n",
      "4               olly UG (haftungsbeschränkt)                      )  No entity\n",
      "..                                       ...                    ...        ...\n",
      "464                Muehlhan Deutschland GmbH            Deutschland        ORG\n",
      "465                Muehlhan Deutschland GmbH                   GmbH  No entity\n",
      "466  Harburg-Freudenberger Maschinenbau GmbH  Harburg-Freudenberger        ORG\n",
      "467  Harburg-Freudenberger Maschinenbau GmbH           Maschinenbau        ORG\n",
      "468  Harburg-Freudenberger Maschinenbau GmbH                   GmbH        ORG\n",
      "\n",
      "[469 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T12:10:21.256018Z",
     "start_time": "2024-10-01T12:10:21.224988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "company_names = [\" Opel ist eine tolle deutsche christlich Firma\", \"Volkswagen baut gute Autos\", \"BASF\"]\n",
    "\n",
    "for company_name in company_names:\n",
    "    doc = de_dep_nlp(company_name)\n",
    "    # Extract and print all entities with their labels\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, \"-\", ent.label_)\n"
   ],
   "id": "38242cbdf2103916",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opel - ORG\n",
      "deutsche - MISC\n",
      "Volkswagen - ORG\n",
      "BASF - ORG\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract entities labeled as PERSON\n",
    "filtered_labels = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "# Extract tokens and their part-of-speech tags\n",
    "tokens_pos = [(w.text, w.pos_) for w in doc]\n",
    "\n",
    "# Create a DataFrame for entities\n",
    "df_entities = pd.DataFrame(filtered_labels, columns=['Person'])\n",
    "\n",
    "# Create a DataFrame for tokens and POS tags\n",
    "df_tokens_pos = pd.DataFrame(tokens_pos, columns=['Token', 'POS'])\n",
    "\n",
    "print(\"Entities DataFrame:\")\n",
    "print(df_entities)\n",
    "\n",
    "print(\"\\nTokens and POS Tags DataFrame:\")\n",
    "print(df_tokens_pos)"
   ],
   "id": "76cbea86ce68f887",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# spaCy English + Google Translate",
   "id": "57c02e2684c731de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8a7f3f4a6cacd419",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
