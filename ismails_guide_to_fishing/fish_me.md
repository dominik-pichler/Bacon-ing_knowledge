
## Overview
For a general overview of the history of Topic modelling: <br>
Paper: [The Evolution of Topic Modelling](https://www.researchgate.net/publication/357792535_The_Evolution_of_Topic_Modeling)

## Metrics for Topic Modelling

### 1. Topic Coherence
Topic coherence measures how semantically similar the words within a topic are. It is a crucial metric because it reflects how interpretable the topics are to humans. Common coherence metrics include: 
- **Adjusted Rank Index**:
- **Signal to noise Ratio:**
- **CV Coherence**: Uses normalized pointwise mutual information (NPMI) and cosine similarity to evaluate coherence. However, it has been criticized and is not always recommended for use. 
- **UMass Coherence**: Measures the co-occurrence of words within a topic based on a reference corpus. It is often preferred over CV coherence for its reliability. 
- **UCI Coherence**: Similar to UMass but uses a sliding window approach over the corpus to calculate coherence.

### 2. Perplexity
Perplexity is a statistical measure that evaluates how well a probability model predicts a sample. In topic modeling, it assesses how well the model predicts unseen documents. Lower perplexity indicates a better fit of the model to the data, but it does not necessarily correlate with human interpretability. 

### 3. Reconstruction Error 
This metric is used primarily with models like Non-negative Matrix Factorization (NMF). It measures the difference between the original document-term matrix and the matrix reconstructed from the topic model. Lower reconstruction error indicates a better model fit.
### 4. Rank Biased Overlap (RBO)
RBO is used to compare the similarity between ranked lists of words in topics. It is useful for comparing topics generated by different models or configurations. 

### 5. Stability Metrics 
These metrics, such as Kendall's Weighted Tau, evaluate the stability of topics across different runs or datasets. Stability is important to ensure that the topics are not artifacts of random initialization or specific data samples. 
## 6. Topic Diversity
Topic diversity assesses the variety of topics generated by the model. A diverse set of topics indicates a more comprehensive representation of the dataset. 
## 7. Human Judgment
Despite the availability of quantitative metrics, human judgment remains an essential part of evaluating topic models. Human evaluators can assess the interpretability and relevance of topics, which is often not fully captured by automated metrics. 
## 8. Probability of Held-out Documents 
This metric evaluates the generalization capability of a topic model by estimating the probability of unseen documents given the trained model. It provides insights into how well the model can predict new data.

## Currently established approaches in Topic Modelling: 
### 1.Non-negativ Matrix-Factorization 
### 2. Graph-based Models
### 3. Traditional TM + Modern NLP 
#### 3.1 Biterm Topic Models
#### 3.2 Self-aggregating Topic Models
#### 3.3 Latent Feature LDA and Latent Feature DMM
#### 3.4 Neural Variational Document Model
#### 3.5 Ida2Vec
#### 3.6 Pseudo-document-based Topic Model (PMT)
#### 3.7 Embedding-based Topic Models
#### 3.8 Gibbs Sampling DMM 

#### 3.9 GPUDMM and DPUPDMM

#### 3.10 Distributed Representation-based Expansion (DREx)

#### 3.11 Common Semantics Topic Models (CSTM)

#### 3.12 Word Embedding LDA

#### 3.13 Laplacian DMM 

#### 3.14 CluWords & CluHTM

#### 3.15 Embedded Topic Model and Dynamic embedded Topic Model

#### 3.16 Topic Modeling with BERT

#### 3.17 Topic Noise Models

### 4. Meta-data augmented, Supervised, and Reinforcement-Learning based Models

